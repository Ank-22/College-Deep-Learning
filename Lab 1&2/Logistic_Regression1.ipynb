{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(DriveSave)Logistic_Regression1.ipynb","provenance":[{"file_id":"1BDbg0lHa5Jr-neXBD8caLKnS8xWR6XcR","timestamp":1600330258672}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lnogPNb3vuvv","colab_type":"text"},"source":["[link text](https://)Prediction Function\n","\n","\n","1-calculate predicted output\n","\n","Feature, Weights\n","Sigmoid Function "]},{"cell_type":"code","metadata":{"id":"isQosx9gvlv9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1599113604452,"user_tz":-330,"elapsed":2072,"user":{"displayName":"Diptee Chikmurge","photoUrl":"","userId":"16638101092453898150"}},"outputId":"b0a3bf02-847e-43c1-9b08-08a2378ec389"},"source":["import math\n","import numpy as np\n","def sigmoid(z):\n","  return 1.0 / (1 + np.exp(-z))\n","  \n","def predict(features, weights):\n","  '''\n","  Returns 1D array of probabilities\n","  that the class label == 1\n","  '''\n","  \n","  z=1/(1 + np.exp(-features*weights))\n","  return(z)\n","\n","\n","features=np.array([2,2,7,2, 4,5,5,6,6,1,8,2,7,9,9,5,2,5,9,1,1,6,4,5,5,4,3])\n","weights=np.random.uniform([0,1,3,5,1,2,0,4,5,0,1,2,3,3,1,4,6,7,1,2,1,3,4,4,1,3,3])\n","#weights=np.array([0,0,1,0,1,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0])\n","labels=np.array([0,0,1,0,1,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0]) \n","print(predict(features, weights))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.76656403 0.88079708 0.99999995 0.9698955  0.98201379 0.99577865\n"," 0.96953573 1.         1.         0.63249262 0.99966465 0.92792592\n"," 0.99999966 1.         0.99987661 1.         0.93889773 0.99999729\n"," 0.99987661 0.75297694 0.73105858 0.99999076 0.99999981 0.99999213\n"," 0.99330715 0.99992899 0.99954162]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"muX4Id6BvszM","colab_type":"text"},"source":["Cost Funtion "]},{"cell_type":"code","metadata":{"id":"DjvtyYC7wCtk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599113604455,"user_tz":-330,"elapsed":2028,"user":{"displayName":"Diptee Chikmurge","photoUrl":"","userId":"16638101092453898150"}},"outputId":"484aefbe-5545-4d3f-a906-350401ee7262"},"source":["def cost_function(features, labels, weights):\n","    '''\n","    Using Mean Absolute Error\n","    Features:(100,3)\n","    Labels: (100,1)\n","    Weights:(3,1)\n","    Returns 1D matrix of predictions\n","    Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n","    '''\n","    observations = len(labels)\n","\n","    predictions = predict(features, weights)\n","    #print(predictions)\n","    #Take the error when label=1\n","    class1_cost = -labels*np.log(predictions)\n","\n","    #Take the error when label=0\n","    class2_cost = (1-labels)*np.log(1-predictions)\n","\n","    #Take the sum of both costs\n","    cost = class1_cost - class2_cost\n","\n","    #Take the average cost\n","    cost = cost.sum() / observations\n","\n","    return cost\n","print(cost_function(features, labels, weights))    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["3.361904971788212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"emPltnk31ZG7","colab_type":"text"},"source":["Gradient Descent "]},{"cell_type":"code","metadata":{"id":"C1LYDz9G1XlL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"status":"ok","timestamp":1599113604457,"user_tz":-330,"elapsed":1985,"user":{"displayName":"Diptee Chikmurge","photoUrl":"","userId":"16638101092453898150"}},"outputId":"8f773e00-e4cf-49f5-93e9-550a84eaa20d"},"source":["def update_weights(features, labels, weights, lr):\n","    '''\n","    Vectorized Gradient Descent\n","    Features:(200, 3)\n","    Labels: (200, 1)\n","    Weights:(3, 1)\n","    '''\n","    epoch=5\n","    N=len(features) \n","    predictions = predict(features, weights)\n","    #2 Transpose features from (200, 3) to (3, 200)\n","    # So we can multiply w the (200,1)  cost matrix.\n","    # Returns a (3,1) matrix holding 3 partial derivatives --\n","    # one for each feature -- representing the aggregate\n","    # slope of the cost function across all observations\n","    for i in range(epoch):\n","     for j in features: \n","      # print(features[j])\n","       gradient = np.dot(features[j],  predictions - labels)\n","\n","    #3 Take the average cost derivative for each feature\n","    #4 this function is used to defined the vlaue of grade.\n","       gradient = gradient/N\n","   \n","       #print(\"g\", gradient)\n","       #4 - Multiply the gradient by our learning rate\n","       gradient = gradient.astype('float32')\n","       #weights = weights.astype('float32')\n","       gradient *= lr\n","  \n","       #5 - Subtract from our weights to minimize cost\n","       weights -= gradient\n","     print(\"w\",weights)\n","    return weights\n","      \n","\n","lr=0.001\n","print(update_weights(features, labels, weights, lr))\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["w [0.59106987 0.99605272 2.39135856 1.73191035 1.0000806  1.09269313\n"," 0.68770761 3.66478357 3.65821716 0.54457212 0.99552002 1.27347026\n"," 2.12132831 2.56684782 1.00000055 3.85064536 1.36635264 2.56361685\n"," 0.99551907 1.11117844 0.99672377 1.92759158 3.87557158 2.35040595\n"," 1.00002999 2.38817227 2.55796964]\n","w [0.58763452 0.99210545 2.39135856 1.72756378 1.00016121 1.09271205\n"," 0.68336266 3.66478357 3.65821716 0.5462191  0.99104004 1.26931178\n"," 2.11684683 2.56236633 1.00000111 3.85064536 1.36662647 2.56361687\n"," 0.99103814 1.10780399 0.99344755 1.92311014 3.87557159 2.35040598\n"," 1.00005999 2.38817259 2.55349021]\n","w [0.58419918 0.98815817 2.39135856 1.72321721 1.00024181 1.09273097\n"," 0.6790177  3.66478357 3.65821716 0.54786607 0.98656006 1.2651533\n"," 2.11236535 2.55788485 1.00000166 3.85064536 1.36690029 2.56361688\n"," 0.98655721 1.10442954 0.99017132 1.9186287  3.87557159 2.35040602\n"," 1.00008998 2.3881729  2.54901078]\n","w [0.58076384 0.9842109  2.39135856 1.71887064 1.00032242 1.09274988\n"," 0.67467274 3.66478357 3.65821716 0.54951305 0.98208008 1.26099482\n"," 2.10788387 2.55340337 1.00000221 3.85064536 1.36717412 2.56361689\n"," 0.98207629 1.10105509 0.9868951  1.91414726 3.87557159 2.35040606\n"," 1.00011998 2.38817322 2.54453135]\n","w [0.5773285  0.98026362 2.39135856 1.71452407 1.00040302 1.0927688\n"," 0.67032779 3.66478357 3.65821716 0.55116003 0.97760011 1.25683633\n"," 2.10340239 2.54892189 1.00000276 3.85064536 1.36744795 2.5636169\n"," 0.97759536 1.09768063 0.98361887 1.90966582 3.87557159 2.35040609\n"," 1.00014997 2.38817354 2.54005193]\n","[0.5773285  0.98026362 2.39135856 1.71452407 1.00040302 1.0927688\n"," 0.67032779 3.66478357 3.65821716 0.55116003 0.97760011 1.25683633\n"," 2.10340239 2.54892189 1.00000276 3.85064536 1.36744795 2.5636169\n"," 0.97759536 1.09768063 0.98361887 1.90966582 3.87557159 2.35040609\n"," 1.00014997 2.38817354 2.54005193]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i1rVpoZC17QQ","colab_type":"text"},"source":["Classify and Prediction "]},{"cell_type":"markdown","metadata":{"id":"AXIdLydrfgbK","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"1DSKoiqOfh1T","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"M7X_uX8w2FaW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599113689959,"user_tz":-330,"elapsed":1059,"user":{"displayName":"Diptee Chikmurge","photoUrl":"","userId":"16638101092453898150"}},"outputId":"3e217309-8657-4526-fdf4-3a4cdafda6b3"},"source":["def decision_boundary(prob):\n","  return 1 if (prob >= .8) else 0\n","\n","\n","def classify(predictions):\n","  '''\n","  input  - N element array of predictions between 0 and 1\n","  output - N element array of 0s (False) and 1s (True)\n","  '''\n","  global decision_boundary\n","  decision_boundary = np.vectorize(decision_boundary)\n","  return decision_boundary(predictions)\n","  \n","print(classify(predict(features,weights)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d7OoSZso4REC","colab_type":"text"},"source":["Train\n"]},{"cell_type":"code","metadata":{"id":"_yCMvmUm4Rq4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599113604464,"user_tz":-330,"elapsed":1954,"user":{"displayName":"Diptee Chikmurge","photoUrl":"","userId":"16638101092453898150"}},"outputId":"fbd87941-4b40-4425-ba81-9b9c284c394f"},"source":["def train(features, labels, weights, lr, iters):\n","    cost_history = []\n","\n","    for i in range(iters):\n","        weights = update_weights(features, labels, weights, lr)\n","\n","        #Calculate error for auditing purposes\n","        cost = cost_function(features, labels, weights)\n","        cost_history.append(cost)\n","\n","        # Log Progress\n","        if i % 1000 == 0:\n","            print (\"iter: \" +str(i) + \" cost: \" +str(cost))\n","\n","    \n","    return weights, cost_history\n","iters=10\n","train(features, labels, weights, lr, iters)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w [0.57392095 0.9763352  2.39135856 1.71018331 1.0004835  1.09278771\n"," 0.66599796 3.66478357 3.65821716 0.55279844 0.97312042 1.25269054\n"," 2.09892091 2.54444041 1.00000332 3.85064536 1.36772108 2.56361691\n"," 0.97311455 1.09432031 0.98035713 1.90518439 3.87557159 2.35040613\n"," 1.00017994 2.38817386 2.53557264]\n","w [0.57051341 0.97240678 2.39135856 1.70584254 1.00056398 1.09280662\n"," 0.66166814 3.66478357 3.65821716 0.55443685 0.96864074 1.24854474\n"," 2.09443943 2.53995893 1.00000387 3.85064536 1.3679942  2.56361693\n"," 0.96863375 1.09095998 0.9770954  1.90070295 3.87557159 2.35040616\n"," 1.00020991 2.38817418 2.53109336]\n","w [0.56710587 0.96847835 2.39135856 1.70150178 1.00064446 1.09282553\n"," 0.65733832 3.66478357 3.65821716 0.55607526 0.96416105 1.24439895\n"," 2.08995795 2.53547744 1.00000442 3.85064536 1.36826733 2.56361694\n"," 0.96415294 1.08759965 0.97383366 1.89622152 3.87557159 2.3504062\n"," 1.00023988 2.3881745  2.52661407]\n","w [0.56369833 0.96454993 2.39135856 1.69716102 1.00072493 1.09284444\n"," 0.6530085  3.66478357 3.65821716 0.55771366 0.95968137 1.24025315\n"," 2.08547647 2.53099596 1.00000498 3.85064536 1.36854045 2.56361695\n"," 0.95967214 1.08423932 0.97057192 1.89174009 3.87557159 2.35040623\n"," 1.00026986 2.38817481 2.52213479]\n","w [0.56029079 0.96062151 2.39135856 1.69282025 1.00080541 1.09286335\n"," 0.64867868 3.66478357 3.65821716 0.55935207 0.95520168 1.23610736\n"," 2.08099499 2.52651448 1.00000553 3.85064536 1.36881358 2.56361696\n"," 0.95519133 1.08087899 0.96731018 1.88725865 3.87557159 2.35040627\n"," 1.00029983 2.38817513 2.5176555 ]\n","iter: 0 cost: 3.271953117872794\n","w [0.55691132 0.95671242 2.39135856 1.68848553 1.00088576 1.09288225\n"," 0.64436554 3.66478357 3.65821716 0.56098198 0.95072235 1.23197466\n"," 2.07651351 2.522033   1.00000608 3.85064536 1.36908601 2.56361698\n"," 0.95071068 1.07753285 0.96406298 1.88277722 3.87557159 2.3504063\n"," 1.00032978 2.38817545 2.51317637]\n","w [0.55353185 0.95280332 2.39135856 1.6841508  1.00096611 1.09290115\n"," 0.64005241 3.66478357 3.65821716 0.56261188 0.94624302 1.22784197\n"," 2.07203203 2.51755152 1.00000664 3.85064536 1.36935843 2.56361699\n"," 0.94623002 1.0741867  0.96081577 1.8782958  3.87557159 2.35040634\n"," 1.00035973 2.38817577 2.50869724]\n","w [0.55015238 0.94889423 2.39135856 1.67981608 1.00104646 1.09292005\n"," 0.63573927 3.66478357 3.65821716 0.56424178 0.94176369 1.22370928\n"," 2.06755055 2.51307004 1.00000719 3.85064536 1.36963086 2.563617\n"," 0.94174937 1.07084056 0.95756857 1.87381437 3.87557159 2.35040637\n"," 1.00038968 2.38817609 2.50421811]\n","w [0.54677291 0.94498514 2.39135856 1.67548135 1.00112681 1.09293895\n"," 0.63142614 3.66478357 3.65821716 0.56587169 0.93728436 1.21957659\n"," 2.06306907 2.50858855 1.00000774 3.85064536 1.36990328 2.56361701\n"," 0.93726871 1.06749441 0.95432136 1.86933294 3.8755716  2.35040641\n"," 1.00041962 2.3881764  2.49973897]\n","w [0.54339344 0.94107604 2.39135856 1.67114662 1.00120716 1.09295785\n"," 0.627113   3.66478357 3.65821716 0.56750159 0.93280503 1.2154439\n"," 2.05858759 2.50410707 1.00000829 3.85064536 1.37017571 2.56361702\n"," 0.93278806 1.06414827 0.95107416 1.86485151 3.8755716  2.35040644\n"," 1.00044957 2.38817672 2.49525984]\n","w [0.54004229 0.93718675 2.39135856 1.66681818 1.00128739 1.09297674\n"," 0.62281824 3.66478357 3.65821716 0.56912305 0.92832612 1.21132473\n"," 2.05410611 2.49962559 1.00000885 3.85064536 1.37044744 2.56361704\n"," 0.92830759 1.06081637 0.94784152 1.86037009 3.8755716  2.35040648\n"," 1.0004795  2.38817704 2.49078087]\n","w [0.53669115 0.93329746 2.39135856 1.66248973 1.00136761 1.09299563\n"," 0.61852347 3.66478357 3.65821716 0.57074451 0.92384721 1.20720557\n"," 2.04962463 2.49514411 1.0000094  3.85064536 1.37071917 2.56361705\n"," 0.92382712 1.05748446 0.94460889 1.85588867 3.8755716  2.35040651\n"," 1.00050943 2.38817736 2.4863019 ]\n","w [0.53334    0.92940817 2.39135856 1.65816129 1.00144783 1.09301452\n"," 0.61422871 3.66478357 3.65821716 0.57236597 0.9193683  1.2030864\n"," 2.04514315 2.49066263 1.00000995 3.85064536 1.3709909  2.56361706\n"," 0.91934665 1.05415256 0.94137626 1.85140725 3.8755716  2.35040655\n"," 1.00053935 2.38817768 2.48182293]\n","w [0.52998886 0.92551888 2.39135856 1.65383284 1.00152806 1.09303341\n"," 0.60993394 3.66478357 3.65821716 0.57398743 0.91488939 1.19896724\n"," 2.04066167 2.48618115 1.00001051 3.85064536 1.37126263 2.56361707\n"," 0.91486618 1.05082066 0.93814363 1.84692583 3.8755716  2.35040658\n"," 1.00056928 2.388178   2.47734397]\n","w [0.52663771 0.92162959 2.39135856 1.64950439 1.00160828 1.0930523\n"," 0.60563917 3.66478357 3.65821716 0.57560889 0.91041048 1.19484807\n"," 2.03618019 2.48169966 1.00001106 3.85064536 1.37153436 2.56361708\n"," 0.91038571 1.04748875 0.93491099 1.84244441 3.8755716  2.35040662\n"," 1.00059921 2.38817831 2.472865  ]\n","w [0.52331513 0.91776058 2.39135856 1.64518248 1.00168838 1.09307119\n"," 0.60136459 3.66478357 3.65821716 0.57722197 0.90593207 1.19074286\n"," 2.03169871 2.47721818 1.00001161 3.85064536 1.37180539 2.5636171\n"," 0.90590547 1.04417115 0.93169297 1.837963   3.8755716  2.35040666\n"," 1.00062911 2.38817863 2.4683862 ]\n","w [0.51999254 0.91389156 2.39135856 1.64086056 1.00176847 1.09309007\n"," 0.59709001 3.66478357 3.65821716 0.57883505 0.90145367 1.18663765\n"," 2.02721723 2.4727367  1.00001217 3.85064536 1.37207643 2.56361711\n"," 0.90142523 1.04085354 0.92847495 1.83348159 3.8755716  2.35040669\n"," 1.00065902 2.38817895 2.46390741]\n","w [0.51666995 0.91002255 2.39135856 1.63653864 1.00184857 1.09310895\n"," 0.59281543 3.66478357 3.65821716 0.58044813 0.89697526 1.18253245\n"," 2.02273576 2.46825522 1.00001272 3.85064536 1.37234747 2.56361712\n"," 0.89694498 1.03753593 0.92525693 1.82900018 3.8755716  2.35040673\n"," 1.00068892 2.38817927 2.45942861]\n","w [0.51334737 0.90615354 2.39135856 1.63221672 1.00192867 1.09312783\n"," 0.58854085 3.66478357 3.65821716 0.58206122 0.89249686 1.17842724\n"," 2.01825428 2.46377374 1.00001327 3.85064536 1.3726185  2.56361713\n"," 0.89246474 1.03421833 0.92203891 1.82451877 3.8755716  2.35040676\n"," 1.00071883 2.38817959 2.45494982]\n","w [0.51002478 0.90228452 2.39135857 1.62789481 1.00200877 1.09314671\n"," 0.58426627 3.66478357 3.65821716 0.5836743  0.88801845 1.17432203\n"," 2.0137728  2.45929226 1.00001382 3.85064536 1.37288954 2.56361715\n"," 0.8879845  1.03090072 0.91882089 1.82003736 3.8755716  2.3504068\n"," 1.00074873 2.3881799  2.45047102]\n","w [0.50673097 0.89843625 2.39135857 1.62357967 1.00208874 1.09316559\n"," 0.58001384 3.66478357 3.65821716 0.58527906 0.88354065 1.17023121\n"," 2.00929132 2.45481077 1.00001438 3.85064536 1.37315989 2.56361716\n"," 0.88350453 1.02759746 0.91561752 1.81555596 3.87557161 2.35040683\n"," 1.00077861 2.38818022 2.44599242]\n","w [0.50343716 0.89458799 2.39135857 1.61926454 1.00216871 1.09318446\n"," 0.5757614  3.66478357 3.65821716 0.58688382 0.87906285 1.1661404\n"," 2.00480984 2.45032929 1.00001493 3.85064536 1.37343024 2.56361717\n"," 0.87902456 1.02429421 0.91241415 1.81107456 3.87557161 2.35040687\n"," 1.0008085  2.38818054 2.44151381]\n","w [0.50014335 0.89073972 2.39135857 1.61494941 1.00224868 1.09320334\n"," 0.57150896 3.66478357 3.65821716 0.58848858 0.87458505 1.16204958\n"," 2.00032836 2.44584781 1.00001548 3.85064536 1.37370058 2.56361718\n"," 0.8745446  1.02099095 0.90921078 1.80659316 3.87557161 2.3504069\n"," 1.00083838 2.38818086 2.4370352 ]\n","w [0.49684954 0.88689145 2.39135857 1.61063428 1.00232865 1.09322221\n"," 0.56725653 3.66478357 3.65821716 0.59009335 0.87010724 1.15795877\n"," 1.99584689 2.44136633 1.00001604 3.85064536 1.37397093 2.56361719\n"," 0.87006463 1.01768769 0.90600741 1.80211176 3.87557161 2.35040694\n"," 1.00086826 2.38818118 2.43255659]\n","w [0.49355574 0.88304319 2.39135857 1.60631915 1.00240862 1.09324108\n"," 0.56300409 3.66478357 3.65821716 0.59169811 0.86562944 1.15386796\n"," 1.99136541 2.43688485 1.00001659 3.85064536 1.37424128 2.56361721\n"," 0.86558466 1.01438443 0.90280404 1.79763036 3.87557161 2.35040697\n"," 1.00089815 2.3881815  2.42807798]\n","w [0.4902909  0.87921613 2.39135857 1.60201107 1.00248847 1.09325995\n"," 0.55877589 3.66478357 3.65821716 0.59329462 0.86115236 1.14979198\n"," 1.98688393 2.43240336 1.00001714 3.85064536 1.37451094 2.56361722\n"," 0.86110504 1.01109558 0.89961535 1.79314897 3.87557161 2.35040701\n"," 1.00092801 2.38818181 2.42359958]\n","w [0.48702606 0.87538908 2.39135857 1.59770299 1.00256831 1.09327881\n"," 0.55454769 3.66478357 3.65821716 0.59489112 0.85667528 1.145716\n"," 1.98240245 2.42792188 1.00001769 3.85064536 1.3747806  2.56361723\n"," 0.85662541 1.00780672 0.89642666 1.78866758 3.87557161 2.35040704\n"," 1.00095787 2.38818213 2.41912117]\n","w [0.48376123 0.87156203 2.39135857 1.59339491 1.00264816 1.09329768\n"," 0.55031949 3.66478357 3.65821716 0.59648763 0.8521982  1.14164002\n"," 1.97792097 2.4234404  1.00001825 3.85064536 1.37505026 2.56361724\n"," 0.85214578 1.00451786 0.89323798 1.78418619 3.87557161 2.35040708\n"," 1.00098773 2.38818245 2.41464276]\n","w [0.48049639 0.86773498 2.39135857 1.58908683 1.00272801 1.09331654\n"," 0.5460913  3.66478357 3.65821716 0.59808414 0.84772112 1.13756404\n"," 1.9734395  2.41895892 1.0000188  3.85064536 1.37531992 2.56361725\n"," 0.84766615 1.001229   0.89004929 1.7797048  3.87557161 2.35040711\n"," 1.00101759 2.38818277 2.41016435]\n","w [0.47723156 0.86390793 2.39135857 1.58477875 1.00280785 1.0933354\n"," 0.5418631  3.66478357 3.65821716 0.59968065 0.84324404 1.13348807\n"," 1.96895802 2.41447744 1.00001935 3.85064536 1.37558959 2.56361727\n"," 0.84318652 0.99794014 0.8868606  1.77522341 3.87557161 2.35040715\n"," 1.00104745 2.38818309 2.40568594]\n","w [0.47399587 0.86010255 2.39135857 1.58047799 1.00288757 1.09335426\n"," 0.53766138 3.66478357 3.65821716 0.60126896 0.83876782 1.12942737\n"," 1.96447654 2.40999596 1.00001991 3.85064536 1.37585857 2.56361728\n"," 0.83870731 0.99466573 0.88368663 1.77074204 3.87557161 2.35040718\n"," 1.00107729 2.38818341 2.40120775]\n","w [0.47076019 0.85629718 2.39135857 1.57617724 1.00296729 1.09337312\n"," 0.53345966 3.66478357 3.65821716 0.60285727 0.8342916  1.12536668\n"," 1.95999506 2.40551447 1.00002046 3.85064536 1.37612754 2.56361729\n"," 0.83422809 0.99139132 0.88051265 1.76626066 3.87557161 2.35040722\n"," 1.00110712 2.38818372 2.39672955]\n","w [0.46752451 0.85249181 2.39135857 1.57187648 1.00304701 1.09339197\n"," 0.52925794 3.66478357 3.65821716 0.60444558 0.82981538 1.12130599\n"," 1.95551359 2.40103299 1.00002101 3.85064536 1.37639652 2.5636173\n"," 0.82974888 0.98811692 0.87733868 1.76177929 3.87557162 2.35040726\n"," 1.00113696 2.38818404 2.39225136]\n","w [0.46428883 0.84868643 2.39135857 1.56757573 1.00312673 1.09341083\n"," 0.52505621 3.66478357 3.65821716 0.60603389 0.82533916 1.1172453\n"," 1.95103211 2.39655151 1.00002156 3.85064536 1.3766655  2.56361732\n"," 0.82526967 0.98484251 0.87416471 1.75729791 3.87557162 2.35040729\n"," 1.0011668  2.38818436 2.38777316]\n","w [0.46105314 0.84488106 2.39135857 1.56327497 1.00320646 1.09342968\n"," 0.52085449 3.66478357 3.65821716 0.60762221 0.82086294 1.1131846\n"," 1.94655063 2.39207003 1.00002212 3.85064536 1.37693448 2.56361733\n"," 0.82079045 0.9815681  0.87099073 1.75281653 3.87557162 2.35040733\n"," 1.00119664 2.38818468 2.38329497]\n","w [0.45784677 0.84109782 2.39135857 1.55898183 1.00328605 1.09344853\n"," 0.51668162 3.66478357 3.65821716 0.60920238 0.81638775 1.10913965\n"," 1.94206916 2.38758855 1.00002267 3.85064536 1.37720278 2.56361734\n"," 0.81631174 0.97830818 0.8678315  1.74833517 3.87557162 2.35040736\n"," 1.00122645 2.388185   2.378817  ]\n","w [0.4546404  0.83731459 2.39135857 1.55468868 1.00336565 1.09346738\n"," 0.51250876 3.66478357 3.65821716 0.61078256 0.81191256 1.1050947\n"," 1.93758768 2.38310707 1.00002322 3.85064536 1.37747108 2.56361735\n"," 0.81183303 0.97504826 0.86467228 1.74385381 3.87557162 2.3504074\n"," 1.00125627 2.38818531 2.37433904]\n","w [0.45143403 0.83353135 2.39135857 1.55039553 1.00344524 1.09348622\n"," 0.50833589 3.66478357 3.65821716 0.61236274 0.80743737 1.10104975\n"," 1.9331062  2.37862558 1.00002378 3.85064536 1.37773938 2.56361736\n"," 0.80735433 0.97178835 0.86151305 1.73937245 3.87557162 2.35040743\n"," 1.00128609 2.38818563 2.36986107]\n","w [0.44822766 0.82974812 2.39135857 1.54610239 1.00352484 1.09350507\n"," 0.50416302 3.66478357 3.65821716 0.61394292 0.80296218 1.0970048\n"," 1.92862473 2.3741441  1.00002433 3.85064536 1.37800768 2.56361738\n"," 0.80287562 0.96852843 0.85835382 1.73489109 3.87557162 2.35040747\n"," 1.0013159  2.38818595 2.3653831 ]\n","w [0.44502129 0.82596488 2.39135857 1.54180924 1.00360443 1.09352392\n"," 0.49999015 3.66478357 3.65821716 0.6155231  0.798487   1.09295984\n"," 1.92414325 2.36966262 1.00002488 3.85064536 1.37827598 2.56361739\n"," 0.79839691 0.96526852 0.85519459 1.73040973 3.87557162 2.3504075\n"," 1.00134572 2.38818627 2.36090514]\n","w [0.44184437 0.82220424 2.39135857 1.537524   1.00368391 1.09354276\n"," 0.49584864 3.66478357 3.65821716 0.6170952  0.79401304 1.08893109\n"," 1.91966178 2.36518114 1.00002543 3.85064536 1.3785436  2.5636174\n"," 0.79391882 0.96202314 0.85205013 1.72592839 3.87557162 2.35040754\n"," 1.00137551 2.38818659 2.35642741]\n","w [0.43866745 0.8184436  2.39135857 1.53323875 1.00376338 1.09356159\n"," 0.49170713 3.66478357 3.65821716 0.61866731 0.78953908 1.08490234\n"," 1.9151803  2.36069966 1.00002599 3.85064536 1.37881123 2.56361741\n"," 0.78944073 0.95877776 0.84890568 1.72144705 3.87557162 2.35040757\n"," 1.00140531 2.38818691 2.35194969]\n","w [0.43549054 0.81468296 2.39135857 1.5289535  1.00384285 1.09358043\n"," 0.48756562 3.66478357 3.65821716 0.62023941 0.78506512 1.08087358\n"," 1.91069883 2.35621818 1.00002654 3.85064536 1.37907885 2.56361742\n"," 0.78496264 0.95553238 0.84576122 1.7169657  3.87557162 2.35040761\n"," 1.0014351  2.38818722 2.34747197]\n","w [0.43231362 0.81092231 2.39135857 1.52466826 1.00392232 1.09359927\n"," 0.48342411 3.66478357 3.65821716 0.62181152 0.78059117 1.07684483\n"," 1.90621735 2.35173669 1.00002709 3.85064536 1.37934647 2.56361744\n"," 0.78048455 0.952287   0.84261676 1.71248436 3.87557162 2.35040764\n"," 1.0014649  2.38818754 2.34299425]\n","w [0.4291367  0.80716167 2.39135857 1.52038301 1.00400179 1.09361811\n"," 0.4792826  3.66478357 3.65821716 0.62338363 0.77611721 1.07281608\n"," 1.90173587 2.34725521 1.00002765 3.85064536 1.3796141  2.56361745\n"," 0.77600646 0.94904162 0.83947231 1.70800302 3.87557163 2.35040768\n"," 1.00149469 2.38818786 2.33851652]\n","w [0.42598936 0.80342407 2.39135857 1.51610596 1.00408114 1.09363694\n"," 0.47517508 3.66478357 3.65821716 0.62494772 0.77164472 1.06880399\n"," 1.8972544  2.34277373 1.0000282  3.85064536 1.37988105 2.56361746\n"," 0.77152913 0.94581082 0.83634264 1.70352169 3.87557163 2.35040771\n"," 1.00152446 2.38818818 2.33403906]\n","w [0.42284201 0.79968646 2.39135857 1.51182892 1.00416049 1.09365577\n"," 0.47106757 3.66478357 3.65821716 0.62651181 0.76717224 1.0647919\n"," 1.89277293 2.33829225 1.00002875 3.85064536 1.380148   2.56361747\n"," 0.76705179 0.94258002 0.83321298 1.69904037 3.87557163 2.35040775\n"," 1.00155423 2.3881885  2.3295616 ]\n","w [0.41969467 0.79594886 2.39135857 1.50755187 1.00423983 1.0936746\n"," 0.46696005 3.66478357 3.65821716 0.6280759  0.76269975 1.0607798\n"," 1.88829145 2.33381077 1.0000293  3.85064536 1.38041495 2.56361749\n"," 0.76257446 0.93934922 0.83008332 1.69455905 3.87557163 2.35040778\n"," 1.00158401 2.38818881 2.32508414]\n","w [0.41654733 0.79221125 2.39135857 1.50327483 1.00431918 1.09369342\n"," 0.46285253 3.66478357 3.65821716 0.62963999 0.75822727 1.05676771\n"," 1.88380998 2.32932929 1.00002986 3.85064536 1.3806819  2.5636175\n"," 0.75809713 0.93611842 0.82695366 1.69007773 3.87557163 2.35040782\n"," 1.00161378 2.38818913 2.32060668]\n","w [0.41339998 0.78847365 2.39135857 1.49899778 1.00439853 1.09371225\n"," 0.45874502 3.66478357 3.65821716 0.63120409 0.75375478 1.05275562\n"," 1.8793285  2.3248478  1.00003041 3.85064536 1.38094885 2.56361751\n"," 0.75361979 0.93288762 0.823824   1.6855964  3.87557163 2.35040785\n"," 1.00164355 2.38818945 2.31612921]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(array([0.41339998, 0.78847365, 2.39135857, 1.49899778, 1.00439853,\n","        1.09371225, 0.45874502, 3.66478357, 3.65821716, 0.63120409,\n","        0.75375478, 1.05275562, 1.8793285 , 2.3248478 , 1.00003041,\n","        3.85064536, 1.38094885, 2.56361751, 0.75361979, 0.93288762,\n","        0.823824  , 1.6855964 , 3.87557163, 2.35040785, 1.00164355,\n","        2.38818945, 2.31612921]),\n"," [3.271953117872794,\n","  3.22709999081434,\n","  3.1823338249151853,\n","  3.137658829447992,\n","  3.09307946413032,\n","  3.048600414527351,\n","  3.0042267298903393,\n","  2.9599636479584914,\n","  2.9158167639660673,\n","  2.8717919678415638])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"iJHxYQGx44LA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"status":"error","timestamp":1599113605054,"user_tz":-330,"elapsed":2528,"user":{"displayName":"Diptee Chikmurge","photoUrl":"","userId":"16638101092453898150"}},"outputId":"fa3fb4c4-1e4c-4b59-9a02-088e3bf270a7"},"source":["def accuracy(predicted_labels, actual_labels):\n","    diff = predicted_labels - actual_labels\n","    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))\n","\n","\n","def plot_decision_boundary(trues, falses):\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","\n","    no_of_preds = len(trues) + len(falses)\n","\n","    ax.scatter([i for i in range(len(trues))], trues, s=25, c='b', marker=\"o\", label='Trues')\n","    ax.scatter([i for i in range(len(falses))], falses, s=25, c='r', marker=\"s\", label='Falses')\n","\n","    plt.legend(loc='upper right');\n","    ax.set_title(\"Decision Boundary\")\n","    ax.set_xlabel('N/2')\n","    ax.set_ylabel('Predicted Probability')\n","    plt.axhline(.5, color='black')\n","    plt.show()\n","\n","print(accuracy(predict(features, weights),labels))\n","import matplotlib.pyplot as plt\n","plot_decision_boundary(trues, falses)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-21a77ab562cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'trues' is not defined"]}]}]}